output_dir: ./ # /tmp may be deleted by your system. Change it to your preference.

# Tokenizer
tokenizer:
  _component_: torchtune.models.llama3.llama3_tokenizer
  path: huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/tokenizer.model
  max_seq_len: null

# Dataset
dataset:
  _component_: torchtune.datasets.instruct_dataset
  packed: False  # True increases speed
  source: openai/gsm8k
  name: main
  split: train #[90%:] #val is in the first 10%
  column_map:
    input: question
    output: answer
  #new_system_prompt: "In your answer tokens, the corresponding softmax probabilities are concatenated right after each token. The probability is multiplied by ten and rounded to the nearest integer. Make use of this information to answer questions."
seed: null
shuffle: True

# Model Arguments
model:
  _component_: torchtune.models.llama3_1.llama3_1_8b

checkpointer:
  _component_: torchtune.training.FullModelHFCheckpointer
  checkpoint_dir: huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/
  checkpoint_files: [
    model-00001-of-00004.safetensors,
    model-00002-of-00004.safetensors,
    model-00003-of-00004.safetensors,
    model-00004-of-00004.safetensors
  ]
  recipe_checkpoint: null
  output_dir: ${output_dir}/checkpoints/${job_id}
  model_type: LLAMA3
resume_from_checkpoint: False

# Fine-tuning arguments
batch_size: 8
epochs: 2
optimizer:
  _component_: torch.optim.AdamW
  lr: 1e-4
  weight_decay: 0.01
lr_scheduler:
  _component_: torchtune.training.lr_schedulers.get_cosine_schedule_with_warmup
  num_warmup_steps: 64
loss:
  _component_: torchtune.modules.loss.CEWithChunkedOutputLoss
max_steps_per_epoch: null
gradient_accumulation_steps: 4  # Use to increase effective batch size
optimizer_in_bwd: False  # True saves memory. Requires gradient_accumulation_steps=1
compile: False  # torch.compile the model + loss, True increases speed + decreases memory

# Training env
device: cuda:0

# Memory management
enable_activation_checkpointing: True  # True reduces memory
enable_activation_offloading: False  # True reduces memory
custom_sharded_layers: ['tok_embeddings', 'output']  # Layers to shard separately (useful for large vocab size models). Lower Memory, but lower speed.

# Reduced precision
dtype: bf16

# Logging
metric_logger:
  _component_: torchtune.training.metric_logging.WandBLogger
  project: entropy_intl
  entity: 
  name: ${job_id}
log_every_n_steps: 1
log_peak_memory_stats: True


# Profiler (disabled)
profiler:
  _component_: torchtune.training.setup_torch_profiler
  enabled: False

  #Output directory of trace artifacts
  output_dir: ${output_dir}/profiling_outputs

  #`torch.profiler.ProfilerActivity` types to trace
  cpu: True
  cuda: True

  #trace options passed to `torch.profiler.profile`
  profile_memory: False
  with_stack: False
  record_shapes: True
  with_flops: False

  # `torch.profiler.schedule` options:
  # wait_steps -> wait, warmup_steps -> warmup, active_steps -> active, num_cycles -> repeat
  wait_steps: 5
  warmup_steps: 3
  active_steps: 2
  num_cycles: 1

job_id: 8b_entropy_ans_intl_num_nosysp